/*
 * @Author: Ruochen Cui 
 * @Date: 2024-03-25 01:39:45 
 * @Last Modified by:   Ruochen Cui 
 * @Last Modified time: 2024-03-25 01:39:45 
 */
# 基于HA-DPO和LLaVA的代码库进行训练和测试

## 对 github repo 的理解

`MODEL_ZOO`: 包含了各种预训练的机器学习模型的权重和配置

`Demo`: 可使用图形界面或命令行进行推理, 测试模型效果

`Gradio Web UI`: 基于 Gradio 库构建的Web用户界面

  - controller: 控制器, 管理 Gradio 应用程序的状态和行为. 接收用户请求, 处理用户输入, 调用响应函数执行请求的操作
  - Web Server: Web 服务器, 接收用户的 HTTP 请求并传递给控制器
  - Gradio Web Server: Gradio Web 服务器, 与 Web 服务器相似, 专门用于托管 Gradio Web 应用程序. 通过调用 Gradio 库中的函数来启动 Web 服务器
  - SGLang Worker: SGLang 工作器, 用于定义模型和界面间通信的语言
  - Model Worker: 模型工作器, 是一个独立的进程或线程，负责加载、运行和管理模型, 启动模型工作器后可以接收输入并产生输出

`CLI Inference`: CLI Inference是指通过命令行界面 (CLI)进行推断 (Inference) 的过程

`legacy model`: 旧有的模型


## linux 和 vscode 的使用

### linux 常用命令

`cd`, `ls`

`cd ~`: 进入 home 目录

`pwd`: 查看当前目录

`echo $PATH`: 查看当前环境配置

`du -ah`: 显示文件夹内文件及其子文件夹的大小

`df -h`: 查看磁盘使用情况

`ps -ef`: 显示当前运行的所有进程

`nvidia-smi`: 显示当前时刻 GPU 使用情况

`nvitop`: 实时显示 GPU 使用情况 (服务器未安装)

`top`: CPU 实时使用情况, 按 I 显示每个进程占用总资源的百分比

`kill PID`: 杀死进程

`ls -l file name`: 查看上次文件的修改日期和修改人


### vscode 的使用

1. 在服务器上安装 python extension 后, 使用 `ctrl + left mouse` 可以跳转至变量/函数上一次使用处或定义处

2. 快速预览 `ctrl + shift + v`

3.md 快速链接文档内位置, 使用 `语法解读](#train_dpopy-语法解读)`, 加粗和 "." 直接忽略, 空格用 "-" 代替, 任意级标题均用一个 #

4. 安装 vscode-fileheader 插件, 使用 `Ctrl + Alt + I` 在文件头部加入作者和日期信息

5. 安装 nvitop 命令, 


## conda环境搭建

参考[环境配置](/home/cuiruochen/环境配置.md)完成conda环境搭建与配置, 使用以下代码使用环境

```
conda activate hadpo
```

## git版本管理

**_step1._** vscode 安装 git 相关插件

**_step2._** 用户登录 github 账号

1. 用户账号的主目录下生成 ssh 密钥对

```
ssh-keygen -t rsa -b 4096 -C "your_email@example.com"
```

打开生成的公钥文件, 它的默认位置是 `~/.ssh/id_rsa.pub`, 复制公钥内容. 然后登录 GitHub, 转到 "Settings" -> "SSH and GPG keys" -> "New SSH key", 将公钥粘贴到 "Key" 文本框中, 然后点击 "Add SSH key"

2. 在项目文件夹路径下登录账号 (**note**: 这里没有使用--global, 使用的后果未知)

```
git config --global user.name "Your Name"
git config --global user.email "your_email@example.com"
git config --global core.sshCommand "ssh -i /home/user1/.ssh/id_rsa"
```

确保将 `/home/user1/.ssh/id_rsa` 替换为你生成的私钥的路径

3. 尝试连接github

```
ssh -T git@github.com
```

确认连接后, 会提示 "You've successfully authenticated", 表示登陆成功


**_step3._** 将服务器仓库与远程仓库关联

```
git remote add origin https://github.com/421zuoduan/Deep-Learning-for-MLLMs.git
```

如果已经与远程仓库关联, 使用以下指令查看当前关联的远程仓库

```
git remote -v
```

与原有远程仓库删除关联

```
git remote remove origin
```

此时再执行前述关联新仓库的指令


**_step4._** 设置.gitignore

`.gitignore` 文件用以设置哪些文件夹和文件不需要上传, 具体格式详见该文件


**_step4._** git push

创建测试环境分支 `dev`, 并在服务器上切换为该分支

```
git checkout -b dev
```

由于本地是 git clone 自HA-DPO仓库并进行了修改, 这里不进行 git pull 操作, 强制 push

```
git add .
git commit -m "init"
git push origin dev --force
```

push 过程中经常出现网络连接问题, 尝试使用以下代码取消代理. 如果依然出现 fatal 报错, 等一会再push

```
git config --global --unset http.proxy
git config --global --unset https.proxy
```

经常出现的报错有:

```
fatal: unable to access 'https://github.com/user/project.git/': GnuTLS recv error (-110): The TLS connection was non-properly terminated.
```

或

```
fatal: unable to access 'https://github.com/user/project.git/': Failed to connect to github.com port 443 after 131081 ms: Connection timed out
```


**_step5._** commit 的撤销与删除

commit时如果发现有时间过长或大文件没有gitignore, 可以使用插件`Git Graph`撤销commit

| | 是否删除对代码的修改 | 是否删除commit记录 | 是否新增commit记录 |
|:--:|:--:|:--:|:--:|
| Undo Commit | 不会 | 未Push会, 已Push不会 | 不会 |
| Revert Commit | 会 | 不会 | 会 |
| Drop Commit | 会 | 未Push会, 已Push不会 | 不会 |

按下`Ctrl + Shift + P`, 然后搜索`Git: Undo Last Commit`, 即可撤销上一次的commit

**_step6._** 大文件 push 后无法解决之究极方法--删除 repo 和 .git 文件

1. 删repo

2. 打开vscode的 `File-Preferences-Setting`, 搜索 `Exclude`, 删除 `**/.git`, 即可显示 `.git` 文件夹, 删除 .git

3. 从用户登录处重新操作

## 数据集与代码准备

### 数据集准备

依照 [data preparation](ha_dpo/data/data_preparation.md) 进行数据集和测试集的准备

数据集结构如下

```
ha_dpo/data
├── coco2014
│   └── val2014
│       └── ...
├── hadpo
│   └── llava-v1.5
│       ├── desc_data.json
|       └── pope_data.json
├── POPE
│   └── ...
├── shr
│   ├── shr_factual_part1.jsonl
│   ├── shr_factual_part2.jsonl
│   └── val_images_final.json
└── VG
    ├── image_data.json+
    ├── region_descriptions.json
    ├── VG_100K
        └── ...
    └── VG_100K_2
        └── ...
```

### 代码准备

#### DeepSpeed 库介绍

[博客介绍](https://blog.csdn.net/u010751000/article/details/123516433)

[官方文档](https://deepspeed.readthedocs.io/en/latest/)

[配置参数文档](https://www.deepspeed.ai/docs/config-json/)

pytorch, tensorflow, keras 等框架在面向大规模模型编程时不是很方便.

以 pytorch 为例, pytorch 的分布式并行计算框架 (Distributed Data Parallel, 简称 DDP), 仅能使数据并行, 即模型大于显卡显存时, 除非将模型参数拆开到各个 GPU 上, 否则无法使用.

DeepSpeed 是微软开源的框架, 能实现拆散功能, 将模型参数拆散到各个 GPU 上, 实现大模型的计算, 是我们用更少的 GPU 训练更大的模型而不受限于显存. 但是 DeepSpeed 的文档写的不好 (我证明! 沟槽的文档,)

载入模型和编写模型的代码基本相同, DeepSpeed 通过输入参数来启动训练, 使用argparse解析参数

#### LLaVA-1.5



## idea 结构

Summary:

1. causal attention mask 是必须要有的
2. attention mask 是没有必要的


## LLaVA-1.5训练

### 源码更新

#### 更新 Evaluation 代码

由于部分 Evaluation 代码已经更新, 需要手动下载 [eval.zip](https://drive.google.com/file/d/1atZSBBrAX54yYpxtVVW33zFvcnaHeFPy/view?usp=sharing) 放到 `ha_dpo/models/llava-v1_5/playground/data/eval` 路径, 压缩包内的十一个文件夹用作不同测试集.

#### 更新POPE代码

根据 [LLaVA-issue-626](https://github.com/haotian-liu/LLaVA/issues/626), repo 内 [POPE的教程](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#pope) 是对POPE旧版本的指导, 但是代码内已经修改为新版本. LLavA在 [Scripts教程](https://github.com/haotian-liu/LLaVA/blob/main/docs/Evaluation.md#scripts) 内给出 `eval.zip`, 压缩包内含有新版POPE的评估代码与测试集. 按照[此处](更新Evaluation代码)更新测试集和代码即可.


### deepspeed 指令解读

根据[文档](https://github.com/opendatalab/HA-DPO/blob/main/ha_dpo/models/llava-v1_5/README.md#model-training)给出的下述训练指令, 找到 `ha_dpo/models/llava-v1_5/train_dpo.py`. 

```
deepspeed ha_dpo/models/llava-v1_5/train_dpo.py \
    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 0 \
    --deepspeed ha_dpo/models/llava-v1_5/scripts/zero3.json \
    --model_name_or_path liuhaotian/llava-v1.5-7b \
    --version v1 \
    --vg_path ha_dpo/data/VG \
    --desc_data_path ha_dpo/data/hadpo/llava-v1.5/desc_data.json \
    --pope_data_path ha_dpo/data/hadpo/llava-v1.5/pope_data.json \
    --vision_tower openai/clip-vit-large-patch14-336 \
    --mm_projector_type mlp2x_gelu \
    --mm_vision_select_layer -2 \
    --mm_use_im_start_end False \
    --mm_use_im_patch_token False \
    --image_aspect_ratio pad \
    --group_by_modality_length True \
    --bf16 True \
    --output_dir ha_dpo/models/llava-v1_5/checkpoints/{model_name} \
    --num_train_epochs 1 \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 1 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 50000 \
    --save_total_limit 1 \
    --learning_rate 2e-6 \
    --weight_decay 0. \
    --warmup_steps 0 \
    --lr_scheduler_type "cosine" \
    --logging_steps 1 \
    --tf32 True \
    --model_max_length 2048 \
    --gradient_checkpointing True \
    --dataloader_num_workers 4 \
    --lazy_preprocess True \
    --report_to wandb \
    --run_name "llava-v1.5" \
    --beta 0.1
```

`deepspeed` 指令后的参数与 `train_dpo.py` 中的 `ScriptArguments`, `DataArguments`, `ModelArguments` 有关, 这里我摘取部分重要参数作解释:

**1. script**

- **lora_enable**: 是否使用 lora 进行训练
- **deepspeed**: deepspeed configuration path
- **bf16**: default=False, bf16 是对 fp32 单精度浮点数截断数据. 8bit 表示指数，7bit 表示小数
- **fp16**: default=False, 是否使用 fp16 权重, 5bit 表示指数，10bit 表示小数. fp16 是半精度浮点数, fp32 是单精度浮点数. fp16 计算更快但精准度更低
- **output_dir**: 训练 checkpoint 保存路径
- **num_train_epochs**: 训练 epoch 次数
- **per_device_train_batch_size**: 每个 device 训练的 batch
- **per_device_eval_batch_size**: 每个 device 测试的 batch
- **gradient_accumulation_steps**: default=4, 梯度累积的steps, 每 N 个batch更新一次参数, 实现类似于相同显存扩大 batch_size 的效果. 这是一种时间换空间的处理方法. [参考博客](https://zhuanlan.zhihu.com/p/595716023)
- **evaluation_strategy**: default=no
- **save_strategy**: 保存策略, default=steps
- **save_steps**: 保存参数的频率, default=-1
- **save_total_limit**: default=1, 被保存的模型的参数的数量
- **learning_rate**: 学习率 2e-6
- **weight_decay**: 
- **warmup_steps**: warmup_steps的数量
- **lr_scheduler_type**: 学习率优化器的种类
- **logging_steps**: 记录日志的频率
- **tf32**: 是否使用 tf32
- **model_max_length**: 句子的最大长度, 少pad多截断
- **gradient_checkpointing**: 是否使用梯度保存点. [博客](https://zhuanlan.zhihu.com/p/602473031), 不保存中间节点的梯度, 在反向传播时重新计算这部分节点的梯度, 是一种用时间换空间的方法.
- **dataloader_num_workers**: dataloader的worker进程数量, 经验设置值是自己电脑/服务器的CPU核心数. worker用来将 batch 取到 RAM 中, 数量为0时 RAM 直接找和取 batch. [博客](https://blog.csdn.net/qq_28057379/article/details/115427052)
- **report_to**: default=wandb, 用于指定要将训练结果和日志报告到的不同日记集成平台
- **run_name**: 跑的模型的名字, 这里要改成自己命名的模型
- **beta**: DPO loss的 beta 值

**2. model**

- **model_name_or_path**: 模型名称/路径, 指向参数文件路径
- **version**: default=v0, 代码内仅区分v0, v5与其他
- **vision_tower**: 属于 linear 层, 在 HA-DPO 代码中要训练
- **mm_projector_type**: projector default linear
- **mm_vision_select_layer**: default=-1 for the last layer
- **mm_use_im_start_end**: instruction tune 时设置为 False
- **mm_use_im_patch_token**: instruction tune 时设置为 False

**3. data**

- **vg_path**: VG数据集路径
- **desc_data_path**: desc_data.json 文件路径
- **pope_data_path**: pope_data.json 文件路径
- **image_aspect_ratio**: default=square
- **group_by_modality_length**: default=False
- **lazy_preprocess**


### deepspeed 指令修改 (unfinished)

需要注意的是, HA-DPO 和 LLaVA 都有使用 lora 训练的教程, 我的实验需要在 backbone 和 head 间加入模块训练. 所以我们作出以下调整:


**1. script**

需要指定:

    

不需要指定:

    lora_enable False


**2. model**

需要指定

    

不需要指定

    freeze_backbone False
    tune_mm_mlp_adapter True

**3. data**





### train_dpo.py 语法解读

deepspeed 库的代码偏向工程, 这里写一些 Python 里的用法

1. `os.environ` 的使用

    [参考博客](https://blog.csdn.net/happyjacob/article/details/109279118)

    `os.environ` 是一个环境变量的字典. 使用以下代码创建自己的环境变量:

    ```
    os.environ["WANDB_PROJECT"]="ha-dpo"
    ```

2. `local_rank` 指定使用哪个 GPU 

3. `@dataclass` 修饰器

    [参考博客1](https://zhuanlan.zhihu.com/p/59657729), [参考博客2](https://zhuanlan.zhihu.com/p/59658598)

    本质是装饰器, 可以给函数动态地增加功能. 以下是 dataclass 装饰器带来的变化：

    - 无需定义__init__, 将值赋给self，dataclass负责处理它

    - 以更加易读的方式预先定义了成员属性和类型提示. 例如, 能轻松知道 val 是 int 类型

4. `*args` 和 `**kwargs`的用法

    [参考博客](https://blog.csdn.net/GODSuner/article/details/117961990), 详见博客

    *args 表示任何多个无名参数, 本质是一个 tuple

    **kwargs 表示关键字参数, 本质是一个 dict

    同时使用时必须要求 *args 参数列要在 **kwargs 前面 (因为位置参数在关键字参数的前面)

5. `@property` 的用法

    把一个方法变成属性. 可以让调用者写出简短的代码，同时保证对参数进行必要的检查

6. `__len__` 魔法方法

    加入 `def __len__(self)` 方法后, 可以直接使用 `len(class_name)` 来获取一个字符串/列表/...的长度, 返回值取决于该方法的返回值

7. `__getitem__` 魔法方法

    `def __getitem__(self, key)` 方法返回所给键对应的值

8. `isinstance()` 函数

    isinstance(a, str). 如果 a 是 str, 返回 True, 例如 a=2 则返回 False

9. `__call__` 方法

    为了将一个类实例当做函数调用, 我们需要在类中实现 `__call__()` 方法. 也就是要在类中实现如下方法: `def __call__(self, *args)`. 这个方法接受一定数量的变量作为输入

10. `ABC 抽象类`

    ABC 是 abc 模块中的一个类，用于定义抽象基类 (Abstract Base Class). ABC 是 abc 模块的一个核心类, 它是所有抽象基类的基类. 抽象基类是一种特殊的类, 它只包含抽象方法和抽象属性的定义, 而没有具体的实现. 抽象基类主要用于定义接口和约束子类的行为, 而不是提供具体的实现. 通过抽象基类, 可以强制子类实现特定的方法和属性, 从而确保子类的一致性和兼容性.
    
    要定义一个抽象基类, 可以继承 ABC 类, 并使用 @abstractmethod 装饰器声明抽象方法. 子类必须实现抽象基类中的所有抽象方法, 否则在实例化子类时会引发 TypeError 异常

11. `@abstractmethod` 

    `@abstractmethod` 是 Python 中的一个装饰器, 用于声明抽象方法. 抽象方法是一个在抽象类中定义的方法, 它只有方法签名而没有具体实现. 在 Python 中, 抽象方法是通过 abc 模块中的 ABC 基类和 abstractmethod 装饰器来定义的. 使用 `@abstractmethod` 装饰器修饰的方法必须在包含该装饰器的类的子类中进行实现. 如果子类没有实现被装饰的方法, 那么在实例化子类时会抛出 TypeError 异常


### train_dpo.py 代码解读

**如有必要, 请读源码**

1. `ModelArguments`, `DataArguments`, `ScriptArguments` 三个类用来接受 deepspeed 指令的参数.

2. `LazySupervisedDataset` 类用以 supervised fine-tuning. 

3. `DataCollatorForSupervisedDataset` 类为 supervised fine-tuning 提供示例

4. `find_all_linear_names` 函数用以找到模型中所有的 linear 层.

5. `make_supervised_data_module` 函数整合 `LazySupervisedDataset` 和 `DataCollatorForSupervisedDataset` 两个类. 

6. `maybe_zero_3` 函数

7. `get_peft_state_maybe_zero_3` 函数

8. `get_peft_state_non_lora_maybe_zero_3` 函数

9. `SaverCallBack` 用以在训练结束时打印出 message 并保存模型参数

    继承自 TrainerCallack, 只有一个新定义方法 on_train_end. 先使用 get_peft_state_non_lora_maybe_zero_3 方法获取 non_lora_state_dict, 然后使用以下代码保存参数
    ```
    torch.save(non_lora_state_dict, os.path.join(args.output_dir, 'non_lora_trainables.bin'))
    ```

10. `setup_llava_model` 方法用以设定 LLaVA 训练时的相关参数, 如是否使用 lora, 是否冻结 backbone.

    - 若在 `os.environ` 环境中没有设置 `LOCAL_RANK`, 设置为默认序号的显卡, 并指定 cuda.
    - 设置好 compute_type 和 bits, 即计算精度和位宽
    - 根据 vision_tower,  加载模型配置, 参数既可以为模型名称, 也可以为具体文件. 此处加载与 model_args.model_name_or_path 有关, 调用 `LlavaLlamaForCausalLM` 类定义 model
    - vision_tower 决定调用 LlavaLlamaForCausalLM 类, 使用方法 LlavaLlamaForCausalLM.from_pretrained 加载 llava 的 backbone 和 head, , tune_mm_mlp_adapter 决定是否训练 mm_projector
    - 根据 freeze_backbone, 设置 backbone, 也即前面得到的 model.model 没有梯度
    - 设置 bits, 设置是否使用梯度累积, 设置是否使用 lora 微调
    - 使用 transformers.AutoTokenizer.from_pretrained 自动分词
    - 根据 model_args.version 设置不同的 pad_token 方法
    - vison_tower 决定使用 model.get_vision_tower() 方法加载 vison_tower. `tune_mm_mlp_adapter` 参数决定了是否微调 变量 mm_projector, 代码如下
    ```
    if model_args.tune_mm_mlp_adapter:
        model.requires_grad_(False)
        for p in model.get_model().mm_projector.parameters():
            p.requires_grad = True
    ```
    - 最后设置一些 args, 返回 model 和 tokenizer

11. `main` 函数为主函数

    - 使用 parser 聚合所有 arguments, 得到 script_args, model_args, data_args 三个设置变量
    - 使用 setup_llava_model 建立模型, 得到 llava_policy 和 tokenizer. 然后 freeze reference model, 设置 llava_ref_model 内的参数没有梯度
    - 使用 make_supervised_data_module 获取数据集
    - 使用 TrainingArguments 初始化训练参数, 使用 LlavaDPOTrainer 初始化 DPO 训练器
    - 使用 `dpo_trainer.train()` 开始训练


### model 文件夹结构

`ha_dpo/models/llava-v1_5/llava/model` 路径下, 可以看到有 `language_model`, `multimodal_encoder`, `multimodal_projector` 三个文件夹. 经检查, 'language_model' 通过继承 `llava_arch.py` 中的类, 调用了 `multimodal_encoder` 和 `multimodal_encoder` 的方法. 

检查发现, 与 `train_dpo.py` 在 `ha_dpo/models/llava-v1_5/llava/train/train.py` 基础上完成, 前者引用了后者的一部分方法


#### language_model 文件夹

`llava_llama.py` 中含有 llava 代码与 head 代码. `LlavaLlamaForCausalLM` 类是 llava, 类中 `self.lm_head` 是 head, head 被定义为一个无偏 linear

文件夹下 `mpt/` 和`llava_mpt.py` 是使用 MPT 作为 LLM 进行微调的代码, 这里我们不需要使用.

#### multimodal_encoder 文件夹

内有 builder.py 和 clip_encoder.py 两个文件. 使用前者的 `build_vision_tower` 建立 vision encoder 的模型. 后者只包括 CLIPVisionTower 类, 

CLIPVisionTower 类内包括:

1. __init__ 初始化方法
2. load_model 加载模型方法 (使用 .from_pretrained)
3. feature_select 方法
4. forward 方法得到 vision encoder 的输出

clip_encoder.py [解读](https://blog.csdn.net/Afree_learnC/article/details/135651147)


#### multimodal_projector 文件夹

内有 builder.py 一个文件. 包括 IdentityMap, SimpleResBlock 两个类和 build_vision_projector 一个方法. 返回一个 Sequential

#### llava_arch.py

该文件在 `model` 文件夹中, 定义的 LlavaMetaModel 和 LlavaMetaForCausalLM 被 llava_llama.py 作为父类被引用

##### LlavaMetaModel 类

1. `__init__`: 使用 build_vision_tower 定义 self.vision_tower, 使用build_vision_projector 定义 self.mm_projector
2. `get_vision_tower`: 从 config 中得到 vision_tower 的值
3. `initialize_vision_modules`: 定义 vision_tower 和 mm_projector, 加载相关参数

##### LlvaMeta_ForCausalLM 类

继承自 ABC 类, 使用了 `@abstractmethod` 装饰器, 可以参考 [语法解读](#train_dpopy-语法解读)

1. `get_model`: 空方法, 要子类给出
2. `get_vision_tower`: 利用 get_model 和 get_vision_tower 方法得到 vision_tower
3. `encode_images`: 将 vision_tower 和 mm_projector 分别作为 images 和 image_features 编码至 image_features 并输出
4. `prepare_inputs_labels_for_multimodal`: 很复杂的函数, 没有为 ha-dpo 作特别修改 (TODO: 应该?). 最终输出新的 label
5. `initialize_vision_tokenizer`: 与 deepspeed 设置中 mm_use_im_patch_token 和 mm_use_im_start_end 参数有关, 其值为 True 时发挥作用, 没细读


#### builder.py

只有 load_pretrained_model 一个函数, 在 `model_vqa.py`, `pope_eval.py` 中被调用

1. 以特定 bit 加载模型
   
2. 如果 model_name 中有 'llava', 加载 llava 模型; 否则加载AutoModelForCausalLM 类.
   
    - 加载 llava, 如果名字中有 'lora' 且未提供 model_base 参数, warning 要提供 model_base 参数; 若提供了则从 model_base 中加载 llava, 这里加载 LlavaLlamaForCausalLM 类. 然后加载 llava 模型中额外的参数, 若有则加载 non_lora_trainables.bin 文件, 否则从 huggingface 加载文件. 

    - 加载 llava, 如果名字中没有 lora, 提供 model_base 参数, 从 llava 中加载参数; 若有 mpt 参数, 加载 LlavaMPTForCausalLM 类, 否则加载 LlavaLlamaForCausalLM 类; 再加载 mm_projector 参数. 若没有提供 model_base 参数, 重复上述操作, 但是不加载 mm_projector

    - 不加载 llava, 若提供 model_base 参数, 加载 AutoModelForCausalLM 类, 加载 lora 参数; 否则重复上述操作, 但是不加载 lora 参数

3. 若使用 llava, 加载 vision_tower
   
4. 返回 tokenizer, model, image_processor, context_len


### 修改代码 (dev)

#### train_dpo_post.py

修改训练参数部分:

1. 增加了 `tune_lm_head` 参数和相关的梯度调整
2. 修改了 `freeze_backbone`, `freeze_mm_mlp_adapter`, `tune_mm_mlp_adapter` 参数相关的梯度调整代码


修改保存参数部分:

1. (TODO) SaverCallBack: 改变 if 判断条件



#### base_dpo_trainer_post.py

(TODO) 修改 reference model 相关的代码

1. BasedDPOTrainer 类初始化函数中删除 is_peft_model 和 self.ref_model 定义的代码, 将使用 self.ref_model 的地方改为使用 policy_model 的输出

2. (TODO: check) policy_model 是 `ha_dpo/models/llava-v1_5/llava/model/language_model/llava_llama.py` 的 `LlavaLlamaForCausalLM` 实例, 该实例中有 `get_model` 函数, 使用该方法可以得到 reference model 的模型 (因为没有使用lora)






#### llava_llama_post.py 文件中加入post progress module (dev)

##### LlavaLlamaModel 类

`LlavaLlamaModel` 类定义了 llava 的 backbone, 继承自 `LlavaMetaModel` 和 `LlamaModel`, 父类都是 `ha_dpo/models/llava-v1_5/llava/model/llava_arch.py` 定义的类.

在 `LlavaLlamaForCausalLM` 类继承自 `LlamaForCausalLM` 和 `LlavaMetaForCausalLM`. 

该类的初始化与其父类 `LlamaForCausalLM` 相同, 这个类是 transformers 库中的模块

在 `train_dpo.py` 文件中, 以下代码定义了该类的初始化输入

```
model = LlavaLlamaForCausalLM.from_pretrained(
    model_args.model_name_or_path,
    cache_dir=script_args.cache_dir,
    **bnb_model_from_pretrained_args
)
```


--------------------------- remake ------------------------




1. [llava_arch_with_post_decoder.py √] 修改 `encoder_images`: 返回 vision_tower_features 和 image_features

`llava_arch.py` 的 `LlavaMetaForCausalLM` 类的 `prepare_inputs_labels_for_multimodal` 方法使用 `encode_images` 方法获得 image_features, 这里的 image_features 是 vision encoder 和 adapter 的输出. 

2. [llava_arch_with_post_decoder.py √]修改 `prepare_inputs_labels_for_multimodal` 类: 添加返回值 vision_tower_image_features

`prepare_inputs_labels_for_multimodal` 在 `llava_dpo_trainer.py` 中被调用, 返回的 image_features 在后续代码 `model.forward` 传入 model

3. [llava_llama_post.py √] 修改 `LlavaLlamaForCausalLM` 的 `forward` 函数

    - 添加传入参数 `image_features`: 传入的该参数可以输入进 post_decoder 中!!!!
    - 添加 `prepare_inputs_labels_for_multimodal` 函数的返回值 `image_features_with_inputs_embeds`: 该函数在 `LlavaLlamaForCausalLM` 类和 trainer 文件中都用到了, trainer 中得到了 `input_embeds` 则不需再重复使用该函数
    - 更新 `image_features`: 若 `input_embeds` 为空, 更新 `image_features` 为 `image_features_with_inputs_embeds`
    - 添加 `super.forward` 父类方法的参数 `image_features`: 传入的该参数可以输入进 post_decoder 中!!!!

4. 参考 `modeling_llama.py` 的 `LlamaForCausalLM` 创建新类, 作为新父类
   
    - 添加父类 `LlamaForCausalLM` 的 `forward` 函数参数 `image_features`: 传入的该参数可以输入进 post_decoder 中!!!!
    - 添加 post_decoder 的代码: 在调用 self.model 后添加 post_decoder 模块

5. [llava_dpo_trainer_post.py √] 添加 `llava_dpo_trainer.py` 调用 `prepare_inputs_labels_for_multimodal` 时的参数, 添加 `model.forward` 代码的传入参数


TODO: 修改 `llava_llama_post.py` 内的新父类, 并由其文件夹下的子类继承


#### 增加文件夹 multimodal_post_decoder (dev)

文件夹内有 `builder.py`, `configuration_post_decoder.py`, 'modelling_post_decoder.py', 'post_decoder.py'

##### builder.py

借鉴 CLIP 的 builder.py


##### configuration_post_decoder.py

借鉴 CLIP 的 CLIPVisionConfig, CLIPTextConfig 和 CLIPConfig, 写出 PostDecoderBackboneConfig, PostDecoderVisionConfig, PostDecoderConfig

TODO: 删除 PostDecoderModule 和 PostDecoderModel


##### modeling_post_decoder.py

斗胆扒源码, 写一下 transformers 库 QAQ

TODO list:

1. - [ ] 记得找个女朋友
2. llava_llama_post.py
   1. - [ ] LlavaMetaModel.initialize_vision_modules 需要加上 Post Decoder 的初始化
   2. - [ ] LlavaMetaModel 加上 get_post_decoder 方法
3. **llava_llama_post.py**
   1. - [ ] double check: LlavaLlamaForCausalLM 类在 forward 函数中调用 super().forward, 从而调用 transformers.modeling.llama 也即父类 LlamaForCausalLM 的 forward, 这里的 forward 调用了 LlamaModel 和 self.lm_head. 所以应该修改 LlavaLlamaForCausalLM 类的 forward 函数!
   2. 创建新类, 继承 modeling_llama.py 的 LlamaForCausalLM, 重写其 forward 函数; 改写 LlavaLlamaForCausalLM, 其父类改为前面创建的新类, 调用父类的 forward 创建 model
4. modeling_llama.py -> 建议继承 modeling_llama.py, 在 modeling_post_decoder.py 中添加子类
   1. - [ ] LlamaForCausalLM 增加 get_post_decoder 方法
5. train_dpo_post.py
   1. - [ ] 修改 from llava.model import *
6. llava_arch_with_post_decoder.py
   1. - [ ] LlavaMetaForCausalLM 增加 get_post_decoder 方法
7. 

需要注意 super().forward, 

"
在Python中，super().forward()会调用父类的forward()函数，但它所使用的self是子类的实例。这意味着，当你在子类的forward()函数中使用super().forward()调用父类的方法时，父类的forward()函数中使用的self.model将引用子类中的model。

因此，如果子类和父类都定义了self.model，而在子类的forward()函数中调用了super().forward()，那么父类的forward()函数中使用的self.model将是子类中定义的model。"


##### post_decoder.py






### 测试中的发现

1. 将 SaverCallBack 中的 on_train_end 方法 (经检测, **kwargs 继承了 TrainerCallback 的初始化参数)拿出来测试

```
non_lora_state_dict = get_peft_state_non_lora_maybe_zero_3(
    llava_policy_model.named_parameters()
)
torch.save(non_lora_state_dict, os.path.join(args.output_dir, 'non_lora_trainables.bin'))
```

得到结果

```
non_lora_state_dict {'lm_head.weight': tensor([[-0.0037,  0.0033, -0.0072,  ...,  0.0056, -0.0081,  0.0070],
        [-0.0325,  0.0469, -0.0018,  ..., -0.0204,  0.0171,  0.0347],
        [-0.0127,  0.0029,  0.0189,  ..., -0.0287,  0.0139, -0.0079],
        ...,
        [-0.0276, -0.0204, -0.0023,  ...,  0.0117, -0.0114, -0.0242],
        [ 0.0216,  0.0275,  0.0337,  ...,  0.0074, -0.0092, -0.0047],
        [ 0.0098, -0.0085,  0.0058,  ..., -0.0303, -0.0192,  0.0347]])}
```

检测结果发现, non_lora_state_dict 保存了 lora 以外的被训练的参数. 所以我们可以修改 on_train_end 的代码, 让其只保留 lm_head 的参数

2. llava_policy_model is not PeftModelForCausalLM, 这在 on_train_end 函数中可能有用

3. on_train_end 函数传入的参数中, args 即 TrainingArguments; state 是 TrainerState, 输出如下

```
TrainerState(epoch=0.9936305732484076, global_step=39, max_steps=39, num_train_epochs=1, total_flos=0.0, log_history=[{'loss': 0.6931, 'learning_rate': 1e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'policy_logps/rejected': -84.550048828125, 'policy_logps/chosen': -124.45401000976562, 'referece_logps/rejected': -84.550048828125, 'referece_logps/chosen': -124.45401000976562, 'logits/rejected': 0.51363205909729, 'logits/chosen': 0.37404561042785645, 'epoch': 0.03, 'step': 1}, {'loss': 0.6931, 'learning_rate': 2e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'policy_logps/rejected': -81.93289184570312, 'policy_logps/chosen': -89.70757293701172, 'referece_logps/rejected': -81.93289184570312, 'referece_logps/chosen': -89.70757293701172, 'logits/rejected': 0.27752095460891724, 'logits/chosen': 0.2900945842266083, 'epoch': 0.05, 'step': 2}, {'loss': 0.6932, 'learning_rate': 1.996397488542526e-06, 'rewards/chosen': -0.00043753389036282897, 'rewards/rejected': -9.089703212339373e-08, 'rewards/accuracies': 0.5, 'rewards/margins': -0.00043744296999648213, 'policy_logps/rejected': -63.038089752197266, 'policy_logps/chosen': -84.37345123291016, 'referece_logps/rejected': -63.038089752197266, 'referece_logps/chosen': -84.36907958984375, 'logits/rejected': 0.4221007227897644, 'logits/chosen': 0.4474368393421173, 'epoch': 0.08, 'step': 3}, {'loss': 0.6929, 'learning_rate': 1.9856159103477083e-06, 'rewards/chosen': 0.0008592546219006181, 'rewards/rejected': -0.00030597150907851756, 'rewards/accuracies': 0.625, 'rewards/margins': 0.0011652261018753052, 'policy_logps/rejected': -109.94159698486328, 'policy_logps/chosen': -130.67153930664062, 'referece_logps/rejected': -109.93854522705078, 'referece_logps/chosen': -130.6801300048828, 'logits/rejected': 0.4648786783218384, 'logits/chosen': 0.299144983291626, 'epoch': 0.1, 'step': 4}, {'loss': 0.6928, 'learning_rate': 1.967732946933499e-06, 'rewards/chosen': -0.0004203259013593197, 'rewards/rejected': -0.0007211655611172318, 'rewards/accuracies': 0.5, 'rewards/margins': 0.000300839776173234, 'policy_logps/rejected': -93.66424560546875, 'policy_logps/chosen': -122.27236938476562, 'referece_logps/rejected': -93.65702819824219, 'referece_logps/chosen': -122.26815795898438, 'logits/rejected': 0.39898908138275146, 'logits/chosen': 0.23938868939876556, 'epoch': 0.13, 'step': 5}, {'loss': 0.6929, 'learning_rate': 1.942877445461084e-06, 'rewards/chosen': 0.00021947931963950396, 'rewards/rejected': 0.002029592404142022, 'rewards/accuracies': 0.5625, 'rewards/margins': -0.00181011320091784, 'policy_logps/rejected': -83.2168197631836, 'policy_logps/chosen': -99.57537078857422, 'referece_logps/rejected': -83.23710632324219, 'referece_logps/chosen': -99.57756042480469, 'logits/rejected': 0.30823999643325806, 'logits/chosen': 0.27277296781539917, 'epoch': 0.15, 'step': 6}, {'loss': 0.6927, 'learning_rate': 1.9112284903881357e-06, 'rewards/chosen': -0.002776903100311756, 'rewards/rejected': -0.0009121030452661216, 'rewards/accuracies': 0.5625, 'rewards/margins': -0.0018647999968379736, 'policy_logps/rejected': -93.71253967285156, 'policy_logps/chosen': -81.49445343017578, 'referece_logps/rejected': -93.70343017578125, 'referece_logps/chosen': -81.46668243408203, 'logits/rejected': 0.455757737159729, 'logits/chosen': 0.5035836696624756, 'epoch': 0.18, 'step': 7}, {'loss': 0.6907, 'learning_rate': 1.873014113161188e-06, 'rewards/chosen': 0.0018662810325622559, 'rewards/rejected': -0.0016945981187745929, 'rewards/accuracies': 0.625, 'rewards/margins': 0.003560879034921527, 'policy_logps/rejected': -87.06300354003906, 'policy_logps/chosen': -125.16746520996094, 'referece_logps/rejected': -87.04605865478516, 'referece_logps/chosen': -125.18614196777344, 'logits/rejected': 0.35486769676208496, 'logits/chosen': 0.362351655960083, 'epoch': 0.2, 'step': 8}, {'loss': 0.69, 'learning_rate': 1.828509649243842e-06, 'rewards/chosen': -0.0007126480923034251, 'rewards/rejected': -0.002190458821132779, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0014778110198676586, 'policy_logps/rejected': -88.94588470458984, 'policy_logps/chosen': -129.97119140625, 'referece_logps/rejected': -88.92398071289062, 'referece_logps/chosen': -129.96408081054688, 'logits/rejected': 0.48265203833580017, 'logits/chosen': 0.2811928987503052, 'epoch': 0.23, 'step': 9}, {'loss': 0.6905, 'learning_rate': 1.7780357543184393e-06, 'rewards/chosen': 0.0003093007835559547, 'rewards/rejected': -0.004516135901212692, 'rewards/accuracies': 0.4375, 'rewards/margins': 0.0048254369758069515, 'policy_logps/rejected': -105.90150451660156, 'policy_logps/chosen': -125.69773864746094, 'referece_logps/rejected': -105.85633850097656, 'referece_logps/chosen': -125.70083618164062, 'logits/rejected': 0.3503667116165161, 'logits/chosen': 0.26132479310035706, 'epoch': 0.25, 'step': 10}, {'loss': 0.6885, 'learning_rate': 1.7219560939545242e-06, 'rewards/chosen': 0.005014353897422552, 'rewards/rejected': -0.00885623600333929, 'rewards/accuracies': 0.8125, 'rewards/margins': 0.01387059036642313, 'policy_logps/rejected': -117.62181091308594, 'policy_logps/chosen': -121.9394302368164, 'referece_logps/rejected': -117.53323364257812, 'referece_logps/chosen': -121.98957824707031, 'logits/rejected': 0.41328611969947815, 'logits/chosen': 0.2776685953140259, 'epoch': 0.28, 'step': 11}, {'loss': 0.6867, 'learning_rate': 1.6606747233900813e-06, 'rewards/chosen': 0.0045740846544504166, 'rewards/rejected': -0.007133515551686287, 'rewards/accuracies': 0.875, 'rewards/margins': 0.011707599274814129, 'policy_logps/rejected': -89.01537322998047, 'policy_logps/chosen': -106.61418151855469, 'referece_logps/rejected': -88.94403839111328, 'referece_logps/chosen': -106.65992736816406, 'logits/rejected': 0.33593448996543884, 'logits/chosen': 0.23272699117660522, 'epoch': 0.31, 'step': 12}, {'loss': 0.6862, 'learning_rate': 1.5946331763042866e-06, 'rewards/chosen': 0.006253372877836227, 'rewards/rejected': -0.006706527434289455, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.012959901243448257, 'policy_logps/rejected': -80.37234497070312, 'policy_logps/chosen': -95.29936218261719, 'referece_logps/rejected': -80.30528259277344, 'referece_logps/chosen': -95.36189270019531, 'logits/rejected': 0.2950093448162079, 'logits/chosen': 0.25794562697410583, 'epoch': 0.33, 'step': 13}, {'loss': 0.6827, 'learning_rate': 1.5243072835572316e-06, 'rewards/chosen': 0.005543178413063288, 'rewards/rejected': -0.013054387643933296, 'rewards/accuracies': 0.75, 'rewards/margins': 0.018597565591335297, 'policy_logps/rejected': -67.59086608886719, 'policy_logps/chosen': -83.57432556152344, 'referece_logps/rejected': -67.4603271484375, 'referece_logps/chosen': -83.6297607421875, 'logits/rejected': 0.4135449230670929, 'logits/chosen': 0.3676679730415344, 'epoch': 0.36, 'step': 14}, {'loss': 0.6867, 'learning_rate': 1.4502037448176732e-06, 'rewards/chosen': -0.007544813212007284, 'rewards/rejected': -0.018351715058088303, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.010806901380419731, 'policy_logps/rejected': -83.85504150390625, 'policy_logps/chosen': -98.08462524414062, 'referece_logps/rejected': -83.67152404785156, 'referece_logps/chosen': -98.00918579101562, 'logits/rejected': 0.4581078290939331, 'logits/chosen': 0.41228869557380676, 'epoch': 0.38, 'step': 15}, {'loss': 0.6846, 'learning_rate': 1.3728564777803086e-06, 'rewards/chosen': 0.0063969893380999565, 'rewards/rejected': -0.017278948798775673, 'rewards/accuracies': 0.75, 'rewards/margins': 0.023675940930843353, 'policy_logps/rejected': -86.30068969726562, 'policy_logps/chosen': -128.14739990234375, 'referece_logps/rejected': -86.1279067993164, 'referece_logps/chosen': -128.21136474609375, 'logits/rejected': 0.44671788811683655, 'logits/chosen': 0.32386812567710876, 'epoch': 0.41, 'step': 16}, {'loss': 0.6853, 'learning_rate': 1.2928227712765502e-06, 'rewards/chosen': -0.002931565046310425, 'rewards/rejected': -0.013579980470240116, 'rewards/accuracies': 0.625, 'rewards/margins': 0.010648414492607117, 'policy_logps/rejected': -107.15428161621094, 'policy_logps/chosen': -151.44296264648438, 'referece_logps/rejected': -107.01848602294922, 'referece_logps/chosen': -151.41363525390625, 'logits/rejected': 0.289702832698822, 'logits/chosen': 0.15713229775428772, 'epoch': 0.43, 'step': 17}, {'loss': 0.6829, 'learning_rate': 1.2106792699957262e-06, 'rewards/chosen': -0.011483002454042435, 'rewards/rejected': -0.023006584495306015, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.01152358204126358, 'policy_logps/rejected': -104.768798828125, 'policy_logps/chosen': -126.76557922363281, 'referece_logps/rejected': -104.53873443603516, 'referece_logps/chosen': -126.6507339477539, 'logits/rejected': 0.2916226387023926, 'logits/chosen': 0.1503886729478836, 'epoch': 0.46, 'step': 18}, {'loss': 0.682, 'learning_rate': 1.1270178197468786e-06, 'rewards/chosen': -0.001223772531375289, 'rewards/rejected': -0.027556754648685455, 'rewards/accuracies': 0.875, 'rewards/margins': 0.026332980021834373, 'policy_logps/rejected': -106.55337524414062, 'policy_logps/chosen': -135.01043701171875, 'referece_logps/rejected': -106.27781677246094, 'referece_logps/chosen': -134.99819946289062, 'logits/rejected': 0.33981209993362427, 'logits/chosen': 0.2068430483341217, 'epoch': 0.48, 'step': 19}, {'loss': 0.6785, 'learning_rate': 1.0424412031961483e-06, 'rewards/chosen': 0.013474726118147373, 'rewards/rejected': -0.023076392710208893, 'rewards/accuracies': 0.8125, 'rewards/margins': 0.03655112162232399, 'policy_logps/rejected': -112.37261962890625, 'policy_logps/chosen': -162.78973388671875, 'referece_logps/rejected': -112.14186096191406, 'referece_logps/chosen': -162.9244842529297, 'logits/rejected': 0.3036132752895355, 'logits/chosen': 0.12571151554584503, 'epoch': 0.51, 'step': 20}, {'loss': 0.6816, 'learning_rate': 9.575587968038518e-07, 'rewards/chosen': -0.00843049306422472, 'rewards/rejected': -0.0244206003844738, 'rewards/accuracies': 0.625, 'rewards/margins': 0.015990108251571655, 'policy_logps/rejected': -101.78663635253906, 'policy_logps/chosen': -133.5826416015625, 'referece_logps/rejected': -101.54241943359375, 'referece_logps/chosen': -133.4983367919922, 'logits/rejected': 0.328110009431839, 'logits/chosen': 0.17135846614837646, 'epoch': 0.54, 'step': 21}, {'loss': 0.679, 'learning_rate': 8.729821802531212e-07, 'rewards/chosen': -0.002528667449951172, 'rewards/rejected': -0.023557303473353386, 'rewards/accuracies': 0.75, 'rewards/margins': 0.021028636023402214, 'policy_logps/rejected': -103.90233612060547, 'policy_logps/chosen': -113.60698699951172, 'referece_logps/rejected': -103.6667709350586, 'referece_logps/chosen': -113.58170318603516, 'logits/rejected': 0.26110395789146423, 'logits/chosen': 0.15507884323596954, 'epoch': 0.56, 'step': 22}, {'loss': 0.6783, 'learning_rate': 7.89320730004274e-07, 'rewards/chosen': 0.0020017207134515047, 'rewards/rejected': -0.03702498599886894, 'rewards/accuracies': 0.8125, 'rewards/margins': 0.03902670741081238, 'policy_logps/rejected': -106.00006866455078, 'policy_logps/chosen': -141.07749938964844, 'referece_logps/rejected': -105.62982177734375, 'referece_logps/chosen': -141.09751892089844, 'logits/rejected': 0.3833061754703522, 'logits/chosen': 0.27891916036605835, 'epoch': 0.59, 'step': 23}, {'loss': 0.6779, 'learning_rate': 7.071772287234496e-07, 'rewards/chosen': 0.008536243811249733, 'rewards/rejected': -0.020124513655900955, 'rewards/accuracies': 0.75, 'rewards/margins': 0.02866075560450554, 'policy_logps/rejected': -94.29042053222656, 'policy_logps/chosen': -134.29739379882812, 'referece_logps/rejected': -94.08916473388672, 'referece_logps/chosen': -134.38275146484375, 'logits/rejected': 0.3668748140335083, 'logits/chosen': 0.30059635639190674, 'epoch': 0.61, 'step': 24}, {'loss': 0.6769, 'learning_rate': 6.271435222196914e-07, 'rewards/chosen': 0.002656079363077879, 'rewards/rejected': -0.03884584456682205, 'rewards/accuracies': 0.75, 'rewards/margins': 0.04150192067027092, 'policy_logps/rejected': -121.41925811767578, 'policy_logps/chosen': -137.95562744140625, 'referece_logps/rejected': -121.03080749511719, 'referece_logps/chosen': -137.98219299316406, 'logits/rejected': 0.3347330689430237, 'logits/chosen': 0.24084509909152985, 'epoch': 0.64, 'step': 25}, {'loss': 0.6749, 'learning_rate': 5.497962551823266e-07, 'rewards/chosen': 0.001035141758620739, 'rewards/rejected': -0.037835266441106796, 'rewards/accuracies': 0.8125, 'rewards/margins': 0.03887040913105011, 'policy_logps/rejected': -126.71417236328125, 'policy_logps/chosen': -137.94854736328125, 'referece_logps/rejected': -126.3358154296875, 'referece_logps/chosen': -137.95889282226562, 'logits/rejected': 0.29158705472946167, 'logits/chosen': 0.20681139826774597, 'epoch': 0.66, 'step': 26}, {'loss': 0.675, 'learning_rate': 4.756927164427684e-07, 'rewards/chosen': 0.015063691884279251, 'rewards/rejected': -0.022693803533911705, 'rewards/accuracies': 0.875, 'rewards/margins': 0.037757497280836105, 'policy_logps/rejected': -56.13045883178711, 'policy_logps/chosen': -82.06806945800781, 'referece_logps/rejected': -55.903526306152344, 'referece_logps/chosen': -82.21871185302734, 'logits/rejected': 0.5235930681228638, 'logits/chosen': 0.507910966873169, 'epoch': 0.69, 'step': 27}, {'loss': 0.6718, 'learning_rate': 4.053668236957134e-07, 'rewards/chosen': 0.01972193643450737, 'rewards/rejected': -0.03555621579289436, 'rewards/accuracies': 0.9375, 'rewards/margins': 0.05527815222740173, 'policy_logps/rejected': -98.81847381591797, 'policy_logps/chosen': -148.17166137695312, 'referece_logps/rejected': -98.46292114257812, 'referece_logps/chosen': -148.36886596679688, 'logits/rejected': 0.3507687449455261, 'logits/chosen': 0.16377748548984528, 'epoch': 0.71, 'step': 28}, {'loss': 0.6742, 'learning_rate': 3.393252766099187e-07, 'rewards/chosen': -0.018009120598435402, 'rewards/rejected': -0.04889293015003204, 'rewards/accuracies': 0.75, 'rewards/margins': 0.030883807688951492, 'policy_logps/rejected': -121.80502319335938, 'policy_logps/chosen': -143.41310119628906, 'referece_logps/rejected': -121.31608581542969, 'referece_logps/chosen': -143.23301696777344, 'logits/rejected': 0.3216416835784912, 'logits/chosen': 0.2286527156829834, 'epoch': 0.74, 'step': 29}, {'loss': 0.6725, 'learning_rate': 2.7804390604547556e-07, 'rewards/chosen': 0.014693082310259342, 'rewards/rejected': -0.049523912370204926, 'rewards/accuracies': 0.9375, 'rewards/margins': 0.0642169862985611, 'policy_logps/rejected': -118.98127746582031, 'policy_logps/chosen': -125.40072631835938, 'referece_logps/rejected': -118.48602294921875, 'referece_logps/chosen': -125.54765319824219, 'logits/rejected': 0.3652724623680115, 'logits/chosen': 0.20693936944007874, 'epoch': 0.76, 'step': 30}, {'loss': 0.6719, 'learning_rate': 2.219642456815607e-07, 'rewards/chosen': 0.000882887514308095, 'rewards/rejected': -0.031379882246255875, 'rewards/accuracies': 0.8125, 'rewards/margins': 0.03226277232170105, 'policy_logps/rejected': -90.94058227539062, 'policy_logps/chosen': -102.11872863769531, 'referece_logps/rejected': -90.62678527832031, 'referece_logps/chosen': -102.1275634765625, 'logits/rejected': 0.4760279357433319, 'logits/chosen': 0.3936196565628052, 'epoch': 0.79, 'step': 31}, {'loss': 0.6728, 'learning_rate': 1.7149035075615791e-07, 'rewards/chosen': 0.019836867228150368, 'rewards/rejected': -0.044196031987667084, 'rewards/accuracies': 0.875, 'rewards/margins': 0.0640328973531723, 'policy_logps/rejected': -104.34880065917969, 'policy_logps/chosen': -115.15336608886719, 'referece_logps/rejected': -103.90684509277344, 'referece_logps/chosen': -115.35174560546875, 'logits/rejected': 0.429831326007843, 'logits/chosen': 0.30831682682037354, 'epoch': 0.82, 'step': 32}, {'loss': 0.6737, 'learning_rate': 1.2698588683881184e-07, 'rewards/chosen': 0.0057275621220469475, 'rewards/rejected': -0.044457100331783295, 'rewards/accuracies': 0.875, 'rewards/margins': 0.05018466338515282, 'policy_logps/rejected': -103.74142456054688, 'policy_logps/chosen': -126.363037109375, 'referece_logps/rejected': -103.2968521118164, 'referece_logps/chosen': -126.42031860351562, 'logits/rejected': 0.4544333815574646, 'logits/chosen': 0.36984020471572876, 'epoch': 0.84, 'step': 33}, {'loss': 0.6782, 'learning_rate': 8.877150961186419e-08, 'rewards/chosen': -0.007054938934743404, 'rewards/rejected': -0.054047245532274246, 'rewards/accuracies': 0.8125, 'rewards/margins': 0.04699230566620827, 'policy_logps/rejected': -103.6268539428711, 'policy_logps/chosen': -112.49445343017578, 'referece_logps/rejected': -103.08638000488281, 'referece_logps/chosen': -112.42390441894531, 'logits/rejected': 0.250047504901886, 'logits/chosen': 0.20872479677200317, 'epoch': 0.87, 'step': 34}, {'loss': 0.6728, 'learning_rate': 5.712255453891579e-08, 'rewards/chosen': 0.00784638337790966, 'rewards/rejected': -0.03549359738826752, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.04333997890353203, 'policy_logps/rejected': -108.78980255126953, 'policy_logps/chosen': -126.36561584472656, 'referece_logps/rejected': -108.43486022949219, 'referece_logps/chosen': -126.44408416748047, 'logits/rejected': 0.376885324716568, 'logits/chosen': 0.25856855511665344, 'epoch': 0.89, 'step': 35}, {'loss': 0.6696, 'learning_rate': 3.2267053066501126e-08, 'rewards/chosen': 0.007438767701387405, 'rewards/rejected': -0.027189070358872414, 'rewards/accuracies': 0.875, 'rewards/margins': 0.03462783992290497, 'policy_logps/rejected': -103.11282348632812, 'policy_logps/chosen': -109.08055114746094, 'referece_logps/rejected': -102.84092712402344, 'referece_logps/chosen': -109.15493774414062, 'logits/rejected': 0.29097914695739746, 'logits/chosen': 0.15530091524124146, 'epoch': 0.92, 'step': 36}, {'loss': 0.6723, 'learning_rate': 1.4384089652291543e-08, 'rewards/chosen': 0.012628043070435524, 'rewards/rejected': -0.04706757515668869, 'rewards/accuracies': 0.75, 'rewards/margins': 0.059695616364479065, 'policy_logps/rejected': -82.8853530883789, 'policy_logps/chosen': -109.25733947753906, 'referece_logps/rejected': -82.41468048095703, 'referece_logps/chosen': -109.38362121582031, 'logits/rejected': 0.4522559642791748, 'logits/chosen': 0.3747296631336212, 'epoch': 0.94, 'step': 37}, {'loss': 0.6711, 'learning_rate': 3.6025114574734782e-09, 'rewards/chosen': -0.005076884757727385, 'rewards/rejected': -0.03883861377835274, 'rewards/accuracies': 0.75, 'rewards/margins': 0.033761728554964066, 'policy_logps/rejected': -103.96752166748047, 'policy_logps/chosen': -146.4207763671875, 'referece_logps/rejected': -103.57913208007812, 'referece_logps/chosen': -146.3699951171875, 'logits/rejected': 0.378604918718338, 'logits/chosen': 0.2760201692581177, 'epoch': 0.97, 'step': 38}, {'loss': 0.6725, 'learning_rate': 0.0, 'rewards/chosen': -0.0008349898271262646, 'rewards/rejected': -0.033970028162002563, 'rewards/accuracies': 0.8125, 'rewards/margins': 0.03313504159450531, 'policy_logps/rejected': -108.51647186279297, 'policy_logps/chosen': -132.4811248779297, 'referece_logps/rejected': -108.1767578125, 'referece_logps/chosen': -132.4727783203125, 'logits/rejected': 0.3725810647010803, 'logits/chosen': 0.2791193127632141, 'epoch': 0.99, 'step': 39}, {'train_runtime': 1678.4614, 'train_samples_per_second': 2.987, 'train_steps_per_second': 0.023, 'total_flos': 0.0, 'train_loss': 0.6816164560807056, 'epoch': 0.99, 'step': 39}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None)
```

control 是 TrainerControl, 输出如下

```
TrainerControl(should_training_stop=True, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False)
```

kwargs 如下, 基本包括 model, tokenizer, optimizer, lr_scheduler, train_dataloader 和 eval_dataloader. model 是 LlavaLlamaForCausalLM 的 value, 包括 embed_tokens, layers, norm, vision_tower, mm_projector, lm_head

```
Key: model, Value: LlavaLlamaForCausalLM(
  (model): LlavaLlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
    (vision_tower): CLIPVisionTower(
      (vision_tower): CLIPVisionModel(
        (vision_model): CLIPVisionTransformer(
          (embeddings): CLIPVisionEmbeddings(
            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(577, 1024)
          )
          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                (self_attn): CLIPAttention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  (activation_fn): QuickGELUActivation()
                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      (0): Linear(in_features=1024, out_features=4096, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
Key: tokenizer, Value: LlamaTokenizer(name_or_path='/home/cuiruochen/model/llava-v1.5-7b', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken("<s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'eos_token': AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'unk_token': AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False), 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False)
Key: optimizer, Value: DeepSpeedOptimizerWrapper (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 2e-06
    lr: 0.0
    maximize: False
    weight_decay: 0.0
)
Key: lr_scheduler, Value: <torch.optim.lr_scheduler.LambdaLR object at 0x7fbeddca3550>
Key: train_dataloader, Value: <accelerate.data_loader.DataLoaderShard object at 0x7fbeddca3a30>
Key: eval_dataloader, Value: None
```

4. 检查 on_train_end 的 if isinstance(kwargs['model'], PeftModelForCausalLM):

    transformers 库内的 trainer.py 没有在 def _inner_training_loop 方法中 调用 on_train_end

5. 在 modeling_llama.py 源码中, LlamaForCausalLM 类的 forward 方法调用 self.model 前输出 self.model, self.model 包括了 vision_tower 和 mm_projector

```
LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)
```


6. 在源码 modeling_llama.py 文件中的 LlamaForCausalLM 类 forward 方法中得到 self.model.get_vision_tower 的值如下

```
<bound method LlavaMetaModel.get_vision_tower of LlavaLlamaModel(
  (embed_tokens): Embedding(32000, 4096, padding_idx=0)
  (layers): ModuleList(
    (0-31): 32 x LlamaDecoderLayer(
      (self_attn): LlamaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLUActivation()
      )
      (input_layernorm): LlamaRMSNorm()
      (post_attention_layernorm): LlamaRMSNorm()
    )
  )
  (norm): LlamaRMSNorm()
  (vision_tower): CLIPVisionTower(
    (vision_tower): CLIPVisionModel(
      (vision_model): CLIPVisionTransformer(
        (embeddings): CLIPVisionEmbeddings(
          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
          (position_embedding): Embedding(577, 1024)
        )
        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (encoder): CLIPEncoder(
          (layers): ModuleList(
            (0-23): 24 x CLIPEncoderLayer(
              (self_attn): CLIPAttention(
                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
              )
              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              (mlp): CLIPMLP(
                (activation_fn): QuickGELUActivation()
                (fc1): Linear(in_features=1024, out_features=4096, bias=True)
                (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              )
              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (mm_projector): Sequential(
    (0): Linear(in_features=1024, out_features=4096, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=4096, out_features=4096, bias=True)
  )
)>
```

    self.get_output_embeddings() 的值为 `Embedding(32000, 4096, padding_idx=0)`, self.get_output_embeddings() 的值为 `Linear(in_features=4096, out_features=32000, bias=False)`, self.model.vision_tower 为

```
CLIPVisionTower(
  (vision_tower): CLIPVisionModel(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
        (position_embedding): Embedding(577, 1024)
      )
      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-23): 24 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            )
            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
)
```

7. `llava_arch_post.py` 的 `encode_images` 里可以获得 vision_tower 和 projector 的输出, 得到 vision_tower 输出 shape [_, 576, 1024], image_features 输出 shape [_, 576, 4096]


### 替换 bin 文件

1. 若仅替换 head:

    修改 `ha_dpo/models/llava-v1_5/replace_head_bin.py` 中两个 bin 文件的路径, 运行该文件. 修改 deepspeed 文件中的 `model_name_or_path` 参数路径. 执行 Evaluation 操作




## 训练日志中的问题

1. Could not estimate the number of tokens of the input, floating-point operations will not be computed

    huggingface 上的[讨论](https://discuss.huggingface.co/t/get-warning-could-not-estimate-the-number-of-tokens-of-the-input-floating-point-operations-will-not-be-computed-when-use-a-customize-trainer-and-customize-data-collator/18517)

2. warnings.warn(
/home/cuiruochen/HA-DPO/ha_dpo/trainer/llava_dpo_trainer_origin.py:135: UserWarning: compute_loss is only implemented for DPODataCollatorWithPadding, and you passed a datacollator that is different than DPODataCollatorWithPadding - you might see unexpected behavior. Alternatively, you can implement your own prediction_step method if you are using a custom data collator

3. [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented

4. [WARNING] [stage3.py:1850:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time

5. [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown

6. /home/cuiruochen/anaconda/envs/hadpo/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None























## baseline测试

根据 HA-DPO 给出[教程](ha_dpo/models/llava-v1_5/README.md), 进行 Evaluation

### POPE Evaluation

**_step 1._** 输入以下指令, 

```
torchrun --nproc_per_node 1 --master_port $RANDOM ha_dpo/models/llava-v1_5/pope_eval.py \
    --coco_path ha_dpo/data/coco2014 \
    --pope_path ha_dpo/data/POPE \
    --model-path /home/cuiruochen/model/llava-v1.5-7b \
    --model-base liuhaotian/llava-v1.5-7b \
    --set {random/popular/adv}
```

1. ```--set```: validation sets in POPE, choose ```random/popular/adv```. After inference, the answer file will be generated under the folder of LLaVA.
2. ```--model-path```: path to the the trained adapter weights.
3. ```--model-base```: 使用LLaVA-baseline时, 不设置此项, 同时设置```--model-path```为```liuhaotian/llava-v1.5-7b```
4. ```--nproc_per_node1```: 代表使用几张卡. 详见[博客](https://blog.csdn.net/u012605037/article/details/115294898)


在服务器上我使用的指令为:

```
torchrun --nproc_per_node 8 --master_port $RANDOM ha_dpo/models/llava-v1_5/pope_eval.py \
    --coco_path ha_dpo/data/coco2014 \
    --pope_path ha_dpo/data/POPE \
    --model-path /home/cuiruochen/model/llava-v1.5-7b \
    --set popular
```

这一指令执行时间较长, 使用两张 3090, 各占用显存 15G, 约 18min; 8 卡 3090 约 5 分钟

**note:** 在 LLaVA 给出的 python==3.10 环境中, 在 torchrun 过程中会报错. 原因未知, 怀疑是 llava 环境重名产生报错

**单机多卡**

1. 可以使用`CUDA_VISIBLE_DEVICES`指定使用哪几张显卡. 不使用该指令同时指定`nproc_per_node`大于1, 会默认使用序号为 0 到 nproc_per_node-1 的显卡

2. (官方推荐, 可拓展到多机多卡) 在命令指定的 `pope_eval.py` 文件中可以修改来指定使用哪几张显卡. 值得注意的是, 分布式运行指令 `torch.distributed.launch` 已经被遗弃, 现在都使用torchrun, 详情见[官方文档](https://pytorch.org/docs/stable/elastic/run.html). "Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions". 

使用1.的代码如下

```
CUDA_VISIBLE_DEVIOCES=3,4,5,6 torchrun --nproc_per_node 4 --master_port $RANDOM ha_dpo/models/llava-v1_5/pope_eval.py \
    --coco_path ha_dpo/data/coco2014 \
    --pope_path ha_dpo/data/POPE \
    --model-path /home/cuiruochen/model/llava-v1.5-7b \
    --set random
```


**_step2._** 修改answer和label的路径

将 `ha_dpo/data/POPE/evaluate.py` 中的 `ans_file` 设置为第一问中产生回答文件的地址, 一般为 `ha_dpo/models/llava-v1_5` 路径下的某个jsonl文件. `label_file` 设置为 `ha_dpo/data/POPE/output/coco` 下的文件


**_step 3._** 进行测试

运行以下代码获取POPE结果

```
python ha_dpo/data/POPE/evaluate.py
```



使用 8 卡 3090, 仅训练 head, 自定义 ref_model, 得到结果: 

| Model | method | HA-DPO | Accuracy | Precision | Recall | F1 Score | Yes Ratio (%) |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| LLaVA-1.5-7B | popular | × | 86.23 | 83.28 | 90.67 | 86.82 | 54.43 |
| LLaVA-1.5-7B | popular | √ | 87.03 | 85.45 | 89.27 | 87.32 | 52.23 |
| LLaVA-1.5-7B | random | × | 89.67 | 88.89 | 90.67 | 89.77 | 51.00 |
| LLaVA-1.5-7B | random | √ | 90.10 | 90.78 | 89.27 | 90.02 | 49.17 |
| LLaVA-1.5-7B | adversarial | × | 79.73 | 74.40 | 90.67 | 81.73 | 60.93 |
| LLaVA-1.5-7B | adversarial | √ | 80.80 | 76.34 | 89.27 | 82.30 | 58.47 |


使用 8 卡 3090, 仅训练 head, 未定义 ref_model, 得到结果: 

| Model | method | HA-DPO | Accuracy | Precision | Recall | F1 Score | Yes Ratio (%) |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| LLaVA-1.5-7B | popular | × | 86.23 | 83.28 | 90.67 | 86.82 | 54.43 |
| LLaVA-1.5-7B | popular | √ | 87.03 | 85.45 | 89.27 | 87.32 | 52.23 |
| LLaVA-1.5-7B | random | × | 89.67 | 88.89 | 90.67 | 89.77 | 51.00 |
| LLaVA-1.5-7B | random | √ | 90.10 | 90.78 | 89.27 | 90.02 | 49.17 |
| LLaVA-1.5-7B | adversarial | × | 79.73 | 74.40 | 90.67 | 81.73 | 60.93 |
| LLaVA-1.5-7B | adversarial | √ | 80.80 | 76.34 | 89.27 | 82.30 | 58.47 |



## TODO

1. - [ ] 记得找个女朋友 QWQ
2. 了解代码结构
   1. - [x] train_dpo.py
   2. - [x] large language model 文件夹
3. - [ ] 学习使用 wandb, 在服务器上登录并使用 api 记录
4. - [ ] 查看 LLaVA 的原生 MODEL_ZOO


## maybe useful

[mm_projector issue](https://github.com/haotian-liu/LLaVA/issues/948)







## 几个问题

1. flash-attn 和 xformers

    flash-attn 适合 A100 服务器使用, xformers 适合 RTX4090 使用? 需要寻找资料

2. ZeRO2, ZeRO3, ZeRo_offload

    时间换空间的策略. ZeRO-1是将优化器分片, ZeRO-2是在ZeRO-1的基础上将梯度分片, ZeRO-3是在ZeRO-2的基础上将权重分配训练. 速度 1>2>3>offload

3. 服务器问题
   
   时间不准, 服务器没有 `tree` 环境指令

4. 指定使用哪张显卡
   
   [官方](https://github.com/microsoft/DeepSpeed/issues/662) 不建议使用 `CUDA_VISIBLE_DEVICES=1 python -m`, 建议在 deepspeed 阶段使用 `deepspeed --include localhost:1` 来指定显卡

5.  以下 Warning 未解决

    ```
    [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
    ```

6. 模型量化策略 quantization strategies

    模型量化 (Quantization) 是一种用于通过修改权重的精度来减小大型神经网络 (包括大型语言模型) 大小的技术, 尝试从 16-bit 变为  4-bit/8-bit training. [official implement](https://github.com/haotian-liu/LLaVA/issues/1041)

    根据 LLaVA official scripts, 使用 4-bits 进行 inference 的指令为:

    ```
    python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path /home/cuiruochen/model/llava-v1.5-7b
    ```

7. 估计模型参数, [博客](https://blog.csdn.net/weixin_44292902/article/details/133767448)

    | dtype | 每10亿参数 (1B) 需要占用内存 |
    |:--:|:--:|
    | float32 | 4G |
    | fp16 / bf16 | 2G |
    | int8 | 1G |
    | int4 | 0.5G |


8. PEFT 的使用

    .get_base_model(), 用于从 PEFT 实例中得到具体的模型, 实例中包括了优化器, 微调策略等等

    ```
    from peft import PEFTModel
    # 实例化 PEFT 模型对象
    peft_model = PEFTModel()
    # 获取基础模型
    base_model = peft_model.get_base_model()
    ```

9. Tensor Parallel

    对于 $Y = XA$, $X$ 和 $A$ 的形状分别为 $(a, b)$ 和 $(b, c)$, 有两种切分方式进行分块计算:

    - $A$ 沿着第二维切分为 $k$ 份, 每一份的形状为 $(b ,c_{\text{\split}})$, 每一份放在一个 GPU 上与 $X$ 相乘, 得到 $k$ 个 $(a, c_{\text{split}})$, 最后将各个 GPU 上的结果按照第二维进行顺序拼接 (all_gather 通信操作), 得到最终结果 $Y$


## Acknowledge






